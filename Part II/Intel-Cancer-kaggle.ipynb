{"cells":[{"metadata":{"_uuid":"a215e477-38d7-4a6b-ab5f-09a964624770","_cell_guid":"32fa69cb-aff6-4b8a-8a47-fa960c778e90","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For import model\n!pip install pretrainedmodels\n#!git clone https://github.com/Cadene/pretrained-models.pytorch.git","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfcb45ea-6878-4fe3-9116-b600ec01af62","_cell_guid":"301956a4-f817-4774-b048-30544e45aeb3","trusted":true},"cell_type":"code","source":"import cv2\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom albumentations import (Flip,Rotate,Compose,RandomBrightness,GaussNoise,HorizontalFlip)\nimport glob\nimport random\nfrom sklearn.model_selection import train_test_split\nimport pretrainedmodels\nimport torchvision.models as models\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69647ad0-ce2d-41cd-904e-056c37d6507c","_cell_guid":"192cd08a-5796-47ef-b04f-87cdbc6ce02d","trusted":true},"cell_type":"code","source":"class LeNet(nn.Module):\n\n  def __init__(self):\n    super(LeNet,self).__init__()\n    self.conv1=nn.Conv2d(3,6,7)\n    self.conv2=nn.Conv2d(6,16,7)\n    self.poo1=nn.MaxPool2d(2)\n    self.pool2=nn.MaxPool2d(2)\n    self.fc1=nn.Linear( 41616,200)\n    self.fc2=nn.Linear(200,84)\n    self.fc3=nn.Linear(84,3)\n\n  def forward(self,x):\n    x=self.poo1(F.relu(self.conv1(x),2))\n    x=self.pool2(F.relu(self.conv2(x),2))\n    x=x.view(x.shape[0],-1)\n    x=F.relu(self.fc1(x))\n    x=F.relu(self.fc2(x))\n    x=self.fc3(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc20a773-a45c-4620-a4fa-41b6ba4855b5","_cell_guid":"c39028b1-496b-4cc0-b0d4-9423ef87478f","trusted":true},"cell_type":"code","source":"\nimport pretrainedmodels.utils as utils\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\nclass Kaggle_Cancer(Dataset):\n  def __init__(self, root_path, transform=None,preprocessing=None,resize=216):\n    self.path = root_path\n    self.transform=transform\n    self.preprocessing=preprocessing\n    self.resize=resize\n\n  def __len__(self):\n      return len(self.path)\n  \n\n  def __getitem__(self, idx):\n    p=self.path[idx]\n    image1=plt.imread(p)\n    label=p.split(\"/\")[-2].split(\"_\")[-1]\n    if self.transform:\n      image1=self.transform(image=image1)['image']\n    image1=transforms.ToPILImage()(image1)\n    image1=transforms.ToTensor()(image1)\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n    image1=normalize(image1)\n    return image1,int(label)-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#premit to download model\nimport ssl\n\ntry:\n   _create_unverified_https_context = ssl._create_unverified_context\nexcept AttributeError:\n    # Legacy Python that doesn't verify HTTPS certificates by default\n    pass\nelse:\n    # Handle target environment that doesn't support HTTPS verification\n    ssl._create_default_https_context = _create_unverified_https_context","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5f3fa53-1459-427a-bf5f-393d324f5346","_cell_guid":"8027ec0d-c234-443c-874c-6015770b69b8","trusted":true},"cell_type":"code","source":"#All model\nresnet50 = pretrainedmodels.__dict__[\"resnet50\"](num_classes=1000, pretrained='imagenet')\nresnet50.last_linear=torch.nn.Linear(in_features=2048,out_features=3, bias=True)\nlenet = LeNet()\nresnext=pretrainedmodels.__dict__[\"se_resnext50_32x4d\"](num_classes=1000, pretrained='imagenet')\nresnext.last_linear=torch.nn.Linear(in_features=2048,out_features=3, bias=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e872005-16c3-41ab-a066-64477fb12eb8","_cell_guid":"223d8c97-ab2b-4327-b208-493c5b44f27f","trusted":true},"cell_type":"code","source":"\npath_train=glob.glob(\"/kaggle/input/train1/train/train/*/*\")\n#path_train=glob.glob(\"/kaggle/input/intel-mobileodt-cervical-cancer-screening/train/train/*/*\")\nrandom.shuffle(path_train)\ntemp=[0]*len(path_train)\nXTrain, xVal, _, _ = train_test_split(path_train,temp, test_size = 0.1, random_state = 0)\ntransform=Compose([Rotate(p=0.4,limit=30),RandomBrightness(p=0.7),GaussNoise(p=0.2),HorizontalFlip(p=0.4)])\n\n#define our dataset using our class\ndataset = Kaggle_Cancer(XTrain,transform=transform)\ntraining_set=DataLoader(dataset,batch_size=64,shuffle=True)\n\nval_set = Kaggle_Cancer(xVal,transform=transform)\nval_loader=DataLoader(val_set,batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1197aea5-7785-4ebe-8b5b-efb51c05276b","_cell_guid":"283c5286-1532-4e35-b873-ed4b46abe861","trusted":true},"cell_type":"code","source":"from tqdm import tqdm, notebook\n\n\ndef train(epoch_number,model,optim,loss):\n  model.train()\n  all_loss=0\n  correct=0\n  tqdm_loader=tqdm(training_set)\n  for  index,(img,target) in enumerate(tqdm_loader):\n    img=img.float().cuda()\n    target=target.long().cuda()\n\n    optim.zero_grad()\n    out=model(img)\n    loss1=loss(out,target)\n    loss1.backward()\n    optim.step()\n    all_loss+=loss1.item()\n    avg_loss=all_loss/(index+1)\n    pred=out.argmax(dim=1,keepdim=True)\n    correct+=pred.eq(target.view_as(pred)).sum().item()/len(target)\n\n    avg_acc=correct/(index+1)\n    tqdm_loader.set_description(\"Epoch {} train loss={:4}  acc={:4} \".format(epoch_number,round(avg_loss,4),round(avg_acc,4)))\n  \n  return avg_loss,avg_acc\n\n\ndef validation(epoch_number,model,loss):\n  model.eval()\n  all_loss=0\n  correct=0\n  tqdm_loader=tqdm(val_loader)\n  for index,(img,target) in enumerate(tqdm_loader):\n    img=img.float().cuda()\n    target=target.long().cuda()\n\n    with torch.no_grad():\n      out=model(img)\n      loss1=loss(out,target)\n    \n    all_loss+=loss1.item()\n    avg_loss=all_loss/(index+1)\n\n    pred=out.argmax(dim=1,keepdim=True)\n    correct+=pred.eq(target.view_as(pred)).sum().item()/len(target)\n    avg_acc=correct/(index+1)\n\n    tqdm_loader.set_description(\"Epoch {} validation loss={:4}  acc={:4} \".format(epoch_number,round(avg_loss,4),round(avg_acc,4)))\n\n  return avg_loss,avg_acc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ed45041-31e5-4ca4-9970-90124ba987eb","_cell_guid":"93effa2d-d094-4ebf-9f59-e39bf41a83a0","trusted":true},"cell_type":"code","source":"\n#Train + Val\nlist_model=[resnext,resnet50,lenet]\nepcoh_number=3\nfor model in list_model:\n    list_epoch=[]\n    l1,l2=0,0\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model=model.to(device)\n    loss_cross=torch.nn.CrossEntropyLoss().cuda()\n    adam=torch.optim.Adam(model.parameters(),lr=0.0001)\n    loss_train={\"avg_loss\":[],\"accuracy\":[]}\n    loss_val={\"avg_loss\":[],\"accuracy\":[]}\n    if model is lenet:\n        epcoh_number=13\n    for epoch in range(epcoh_number):\n        list_epoch.append(epoch)\n        l1,l2=train(epoch,model,adam,loss_cross)\n        loss_train[\"avg_loss\"].append(l1)\n        loss_train[\"accuracy\"].append(l2)\n        l1,l2=validation(epoch,model,loss_cross)\n        loss_val[\"avg_loss\"].append(l1)\n        loss_val[\"accuracy\"].append(l2)\n    plt.plot(list_epoch,loss_train[\"accuracy\"],label=\"train\")\n    plt.plot(list_epoch,loss_val[\"accuracy\"],label=\"validation\")\n    # naming the x axis \n    plt.xlabel('Epoch') \n    # naming the y axis \n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()\n\n    plt.plot(list_epoch,loss_train[\"avg_loss\"],label=\"train\")\n    plt.plot(list_epoch,loss_val[\"avg_loss\"],label=\"validation\")\n    plt.xlabel('Epoch') \n    # naming the y axis \n    plt.ylabel('convergence')\n    plt.legend()\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ensemble(nn.Module):\n    \n    def __init__(self,model,w):\n        super(ensemble,self).__init__()\n        self.models=model\n        self.w=w\n        \n    def __call__(self,x):\n        result=[]\n        for w1,m1 in zip(self.w,self.models):\n            m1.eval()\n            with torch.no_grad():\n                y=w1*m1(x)\n                y=F.softmax(y,dim=1)\n                result.append(y.cpu().numpy())\n        result=np.array(result)\n        return result.mean(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w=[0.4,0.4,0.2]\nensemble_model=ensemble(list_model,w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict on test\ndef predict(list_model,list_file,resize=False):\n    y_test=[]\n    for i in range(len(list_file)):\n        image1=cv2.imread(list_file[i])\n        if resize:\n            image1=cv2.resize(image1,(224,224))\n        image1=cv2.cvtColor(image1,cv2.COLOR_BGR2RGB)\n        image1=transforms.ToPILImage()(image1)\n        image1=transforms.ToTensor()(image1)\n        \n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n        image1=normalize(image1)\n        image1 = image1[None, :, :]\n        image1=image1.float().cuda()\n        y_test=ensemble_model(image1)\n        id=list_file[i].split(\"/\")[-1]\n        sample_subm.loc[sample_subm['image_name'] == id, 'Type_1'] = y_test[0,0]\n        sample_subm.loc[sample_subm['image_name'] == id, 'Type_2'] = y_test[0,1]\n        sample_subm.loc[sample_subm['image_name'] == id, 'Type_3'] = y_test[0,2]\n\n#Image here 224*224\nlist_file=glob.glob('/kaggle/input/test224/test_stg2/*')\nsample_subm = pd.read_csv(\"/kaggle/input/intel-mobileodt-cervical-cancer-screening/sample_submission_stg2.csv\")\npredict(ensemble_model,list_file)\n\n#image other size so resize to 224*224 \nlist_file=glob.glob(\"/kaggle/input/intel-mobileodt-cervical-cancer-screening/test/test/*\")\npredict(ensemble_model,list_file,True)\n\n#Result\nsample_subm.to_csv(\"submissionCancer.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}